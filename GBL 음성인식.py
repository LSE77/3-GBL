import sounddevice as sd
import soundfile as sf
import numpy as np
import whisper
import openai
import torchaudio
import torch
from playsound import playsound
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.models.xtts import Xtts

# ğŸ”‘ OpenAI API í‚¤ ì„¤ì • (ë°˜ë“œì‹œ ê°œì¸ í‚¤ë¡œ ë³€ê²½)
openai.api_key = "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"  # í”„ë¡œì íŠ¸ í‚¤ X, ê°œì¸ í‚¤ ì‚¬ìš© ê¶Œì¥

# ì„¤ì •
DURATION = 5
SAMPLERATE = 16000
AUDIO_FILE = "user_audio.wav"
MIC_DEVICE_ID = 2

# 1ï¸âƒ£ ë§ˆì´í¬ ë…¹ìŒ
print("ğŸ¤ ë…¹ìŒ ì‹œì‘... ë§í•˜ì„¸ìš”.")
recording = sd.rec(int(DURATION * SAMPLERATE),
                   samplerate=SAMPLERATE,
                   channels=1,
                   dtype='float32',
                   device=MIC_DEVICE_ID)
sd.wait()

print("ìµœëŒ€ ë³¼ë¥¨:", np.max(np.abs(recording)))
sf.write(AUDIO_FILE, (recording * 32767).astype(np.int16), SAMPLERATE)
print("âœ… ë…¹ìŒ ì™„ë£Œ:", AUDIO_FILE)

# 2ï¸âƒ£ Whisper ìŒì„± ì¸ì‹
print("ğŸ—£ï¸ ìŒì„± ì¸ì‹ ì¤‘...")
whisper_model = whisper.load_model("base")
audio_result = whisper_model.transcribe(AUDIO_FILE)
user_text = audio_result['text']
print("ğŸ‘‰ ì‚¬ìš©ì ì…ë ¥:", user_text)

if user_text.strip() == "":
    print("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: ì…ë ¥ëœ ìŒì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# 3ï¸âƒ£ GPT-4o ì‘ë‹µ ìƒì„±
print("ğŸ¤– GPT ì‘ë‹µ ìƒì„± ì¤‘...")
try:
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” í”„ë¦¬ë Œì²˜ëŸ¼ ì¡°ìš©í•˜ê³  ì°¨ë¶„í•œ ë§íˆ¬ë¡œ ë§í•˜ëŠ” ìºë¦­í„°ì•¼. ê·¸ë¦¬ê³  2ì¤„ ì´ìƒ ë„˜ê¸°ì§€ ë§ˆ."},
            {"role": "user", "content": user_text}
        ]
    )
    reply_text = response.choices[0].message.content.strip()
    print("ğŸ’¬ GPT ì‘ë‹µ:", reply_text)

except Exception as e:
    print("âŒ GPT í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", str(e))
    exit()

# 4ï¸âƒ£ XTTS ìºë¦­í„° ìŒì„± í•©ì„± (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
try:
    print("ğŸ› ï¸ XTTS ëª¨ë¸ ë¡œë”© ì¤‘...")
    config = XttsConfig()
    config.load_json("/home/kimsj/PycharmProjects/pythonProject/TTS/run/training/GPT_XTTS_v2.0_LJSpeech_FT-May-11-2024_12+19PM-dbf1a08a/config.json")
    model = Xtts.init_from_config(config)
    model.load_checkpoint(config,
                          checkpoint_path="/home/kimsj/PycharmProjects/pythonProject/TTS/run/training/GPT_XTTS_v2.0_LJSpeech_FT-May-11-2024_12+19PM-dbf1a08a/best_model_9636.pth",
                          vocab_path="/home/kimsj/PycharmProjects/pythonProject/TTS/run/training/XTTS_v2.0_original_model_files/vocab.json",
                          use_deepspeed=True)
    model.cuda()

    print("ğŸ™ï¸ í™”ì ì„ë² ë”© ìƒì„± ì¤‘...")
    gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=[
        "/home/kimsj/PycharmProjects/pythonProject/240510_whisper/wavs/audio1.wav"
    ])

    print("ğŸ§ ìºë¦­í„° ìŒì„± ìƒì„± ì¤‘...")
    out = model.inference(
        reply_text,
        "ko",
        gpt_cond_latent,
        speaker_embedding,
        temperature=0.7,
    )

    torchaudio.save("output_reply.wav", torch.tensor(out["wav"]).unsqueeze(0), 24000)

except Exception as e:
    print("âŒ XTTS ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", str(e))
    exit()

# 5ï¸âƒ£ ìºë¦­í„° ìŒì„± ì¬ìƒ
print("ğŸ”Š ìºë¦­í„° ìŒì„± ì¬ìƒ ì¤‘...")
try:
    playsound("output_reply.wav")
except Exception as e:
    print("âŒ ìŒì„± ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", str(e))

print("âœ… ëŒ€í™” ì¢…ë£Œ")